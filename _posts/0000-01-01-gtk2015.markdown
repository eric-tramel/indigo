---
title: "Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer Free Energy"
layout: post
date: 2015-12-01
tag: 
- publication
- unsupervised learning
- boltzmann machines
- machine learning
publication: true
conference: true
authors: M. Gabrié, E. W. Tramel, & F. Krzakala
in: Neural and Information Processing Symposium (NIPS)
year: 2015
img: /assets/images/gtk2015.png
---

<div align="center">
<h3>Marylou Gabrié, Eric W. Tramel, & Florent Krzakala</h3>
<a href="http://papers.nips.cc/paper/5788-training-restricted-boltzmann-machine-via-the-thouless-anderson-palmer-free-energy">[Link]</a>
<a href="http://papers.nips.cc/paper/5788-training-restricted-boltzmann-machine-via-the-thouless-anderson-palmer-free-energy.pdf">[PDF]</a>
<a href="http://github.com/sphinxteam/Boltzmann.jl">[Code]</a>
</div>

- - -

![Main Figure]({{ page.img }})
<figcaption class="caption">Estimates of the per-sample log-likelihood over the MNIST test set, normalized by the total number of units, as a function of the number of training epochs. The results for the different training algorithms are plotted in different colors with the same color code used for both panels. <b>Left panel:</b> Pseudo log-likelihood estimate. The difference between EMF algorithms and contrastive divergence algorithms is minimal. <b>Right panel:</b> EMF log-likelihood estimate at 2nd order. The improvement from MF to TAP is clear. Perhaps reasonably, TAP demonstrates an advantage over CD and PCD. Notice how the second-order EMF approximation of L provides less noisy estimates, at a lower computational cost.</figcaption>

<br><br>

***Abstract ---*** Restricted Boltzmann machines are undirected neural networks which have been shown tobe effective in many applications, including serving as initializations fortraining deep multi-layer neural networks. One of the main reasons for their success is the existence of efficient and practical stochastic algorithms, such as contrastive divergence,for unsupervised training. We propose an alternative deterministic iterative procedure based on an improved mean field method from statistical physics known as the Thouless-Anderson-Palmer approach. We demonstrate that our algorithm provides performance equal to, and sometimes superior to, persistent contrastive divergence, while also providing a clear and easy to evaluate objective function. We believe that this strategycan be easily generalized to other models as well as to more accurate higher-order approximations, paving the way for systematic improvements in training Boltzmann machines with hidden units.

### BibTeX Record
```
    conference{GTZ2015,
        Address = {Montreal, Canada},
        Author = {Marylou Gabri{\'e} and Eric W. Tramel and Florent Krzakala},
        Booktitle = {Proc. Conf. on Neural Info. Processing Sys. (NIPS)},
        Month = {June},
        Title = {Training Restricted {B}oltzmann Machines via the {Thouless-Andreson-Palmer} Free Energy},
        Year = {2015}}
```